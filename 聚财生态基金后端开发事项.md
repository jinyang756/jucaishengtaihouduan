# 绿色生态基金虚拟净值模拟系统 - 后端实现方案

## 一、后端架构设计

基于之前设计的规则虚拟库和算法公式，我将构建一个完整的后端系统。采用现代化的微服务架构，确保系统的可扩展性和稳定性。

### 1.1 整体架构图

```
┌─────────────────────────────────────────────────────────────────┐
│                      客户端应用层                                │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│  │ Web前端     │  │ 移动应用    │  │ 第三方系统  │              │
│  └─────────────┘  └─────────────┘  └─────────────┘              │
└───────────────────────────┬─────────────────────────────────────┘
                           │
┌───────────────────────────▼─────────────────────────────────────┐
│                      API网关层                                  │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │ 认证授权 | 请求路由 | 负载均衡 | 限流熔断 | 监控日志    │    │
│  └─────────────────────────────────────────────────────────┘    │
└───────────────────────────┬─────────────────────────────────────┘
                           │
┌───────────────────────────▼─────────────────────────────────────┐
│                      微服务层                                   │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────┐ │
│  │ 用户服务    │  │ 基金服务    │  │ 新闻服务    │  │ 规则服务│ │
│  └─────────────┘  └─────────────┘  └─────────────┘  └─────────┘ │
│                                                                 │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────┐ │
│  │ 计算服务    │  │ 通知服务    │  │ 监控服务    │  │ 分析服务│ │
│  └─────────────┘  └─────────────┘  └─────────────┘  └─────────┘ │
└───────────────────────────┬─────────────────────────────────────┘
                           │
┌───────────────────────────▼─────────────────────────────────────┐
│                      数据持久层                                 │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────┐ │
│  │ MySQL数据库 │  │ Redis缓存   │  │ MongoDB文档 │  │ Elastic │ │
│  └─────────────┘  └─────────────┘  └─────────────┘  └─────────┘ │
└─────────────────────────────────────────────────────────────────┘
```

### 1.2 技术栈选择

| 组件 | 技术选型 | 说明 |
|------|----------|------|
| 开发语言 | Python 3.9+ | 适合数据处理和算法实现 |
| Web框架 | FastAPI | 高性能异步API框架 |
| 数据库 | MySQL 8.0 | 关系型数据存储 |
| 缓存 | Redis 6.2 | 缓存和会话管理 |
| 消息队列 | RabbitMQ | 异步任务处理 |
| 搜索引擎 | Elasticsearch | 新闻内容检索 |
| 容器化 | Docker & Kubernetes | 部署和编排 |
| 监控 | Prometheus + Grafana | 系统监控 |
| 日志 | ELK Stack | 日志收集和分析 |

## 二、核心服务实现

### 2.1 规则服务 (Rule Service)

实现规则虚拟库的管理和执行功能。

```python
# rule_service/models.py
from sqlalchemy import Column, String, JSON, Enum, DateTime
from sqlalchemy.ext.declarative import declarative_base
import enum
from datetime import datetime

Base = declarative_base()

class RuleStatus(str, enum.Enum):
    DRAFT = "draft"
    ACTIVE = "active"
    INACTIVE = "inactive"
    DEPRECATED = "deprecated"

class RuleType(str, enum.Enum):
    KEYWORD = "keyword"
    SENTIMENT = "sentiment"
    IMPACT = "impact"
    ALLOCATION = "allocation"
    EVALUATION = "evaluation"

class Rule(Base):
    __tablename__ = "rules"
    
    id = Column(String(36), primary_key=True)
    fund_type = Column(String(50), nullable=False)
    name = Column(String(100), nullable=False)
    description = Column(String(500))
    rule_type = Column(Enum(RuleType), nullable=False)
    content = Column(JSON, nullable=False)
    status = Column(Enum(RuleStatus), default=RuleStatus.DRAFT)
    priority = Column(Integer, default=5)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    version = Column(String(20), default="1.0")
```

```python
# rule_service/service.py
from fastapi import FastAPI, Depends, HTTPException
from sqlalchemy.orm import Session
from .models import Rule, RuleStatus, RuleType
from .schemas import RuleCreate, RuleUpdate, RuleResponse
from .database import get_db
import uuid

app = FastAPI()

@app.post("/rules", response_model=RuleResponse)
def create_rule(rule: RuleCreate, db: Session = Depends(get_db)):
    """创建新规则"""
    db_rule = Rule(
        id=str(uuid.uuid4()),
        fund_type=rule.fund_type,
        name=rule.name,
        description=rule.description,
        rule_type=rule.rule_type,
        content=rule.content,
        status=RuleStatus.DRAFT,
        priority=rule.priority
    )
    
    db.add(db_rule)
    db.commit()
    db.refresh(db_rule)
    return db_rule

@app.get("/rules/{rule_id}", response_model=RuleResponse)
def get_rule(rule_id: str, db: Session = Depends(get_db)):
    """获取规则详情"""
    rule = db.query(Rule).filter(Rule.id == rule_id).first()
    if not rule:
        raise HTTPException(status_code=404, detail="Rule not found")
    return rule

@app.put("/rules/{rule_id}/status")
def update_rule_status(rule_id: str, status: RuleStatus, db: Session = Depends(get_db)):
    """更新规则状态"""
    rule = db.query(Rule).filter(Rule.id == rule_id).first()
    if not rule:
        raise HTTPException(status_code=404, detail="Rule not found")
    
    rule.status = status
    db.commit()
    return {"message": "Rule status updated successfully"}

@app.get("/rules/fund/{fund_type}")
def get_rules_by_fund_type(fund_type: str, status: RuleStatus = RuleStatus.ACTIVE, db: Session = Depends(get_db)):
    """根据基金类型获取规则"""
    rules = db.query(Rule).filter(
        Rule.fund_type == fund_type,
        Rule.status == status
    ).order_by(Rule.priority.desc()).all()
    return rules
```

### 2.2 新闻服务 (News Service)

实现新闻数据的抓取、存储和分析功能。

```python
# news_service/models.py
from sqlalchemy import Column, String, Text, DateTime, Float
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime

Base = declarative_base()

class NewsSource(Base):
    __tablename__ = "news_sources"
    
    id = Column(String(36), primary_key=True)
    name = Column(String(100), nullable=False)
    url = Column(String(500), nullable=False)
    fund_type = Column(String(50), nullable=False)
    active = Column(Boolean, default=True)
    crawl_interval = Column(Integer, default=30)  # 分钟
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class NewsArticle(Base):
    __tablename__ = "news_articles"
    
    id = Column(String(36), primary_key=True)
    source_id = Column(String(36), nullable=False)
    title = Column(String(500), nullable=False)
    content = Column(Text)
    url = Column(String(500), nullable=False)
    published_at = Column(DateTime, nullable=False)
    crawled_at = Column(DateTime, default=datetime.utcnow)
    sentiment_score = Column(Float)
    relevance_score = Column(Float)
    fund_impact = Column(JSON)
```

```python
# news_service/crawler.py
import requests
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime
import uuid
from .models import NewsArticle
from .database import get_db

class NewsCrawler:
    def __init__(self, source):
        self.source = source
        self.db = next(get_db())
    
    def crawl(self):
        """执行新闻抓取"""
        if self.source.url.endswith('.xml') or 'rss' in self.source.url.lower():
            return self._crawl_rss()
        else:
            return self._crawl_html()
    
    def _crawl_rss(self):
        """抓取RSS feed"""
        feed = feedparser.parse(self.source.url)
        articles = []
        
        for entry in feed.entries:
            # 检查是否已存在
            existing = self.db.query(NewsArticle).filter(
                NewsArticle.url == entry.link
            ).first()
            
            if not existing:
                article = NewsArticle(
                    id=str(uuid.uuid4()),
                    source_id=self.source.id,
                    title=entry.title,
                    content=entry.get('summary', ''),
                    url=entry.link,
                    published_at=datetime(*entry.published_parsed[:6]) if 'published_parsed' in entry else datetime.utcnow()
                )
                articles.append(article)
        
        self.db.add_all(articles)
        self.db.commit()
        return articles
    
    def _crawl_html(self):
        """抓取HTML页面"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        try:
            response = requests.get(self.source.url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # 根据不同网站的结构解析新闻
            # 这里需要针对不同网站编写特定的解析规则
            articles = self._parse_articles(soup)
            return articles
            
        except Exception as e:
            print(f"Error crawling {self.source.url}: {str(e)}")
            return []
    
    def _parse_articles(self, soup):
        """解析文章列表"""
        articles = []
        
        # 这里需要根据具体网站的HTML结构进行解析
        # 示例代码，需要根据实际情况调整
        news_items = soup.find_all('div', class_='news-item')
        
        for item in news_items:
            title_elem = item.find('h3', class_='title')
            link_elem = item.find('a')
            time_elem = item.find('span', class_='time')
            
            if title_elem and link_elem:
                title = title_elem.text.strip()
                url = link_elem.get('href')
                published_at = self._parse_time(time_elem.text.strip()) if time_elem else datetime.utcnow()
                
                # 检查是否已存在
                existing = self.db.query(NewsArticle).filter(
                    NewsArticle.url == url
                ).first()
                
                if not existing:
                    article = NewsArticle(
                        id=str(uuid.uuid4()),
                        source_id=self.source.id,
                        title=title,
                        url=url,
                        published_at=published_at
                    )
                    articles.append(article)
        
        self.db.add_all(articles)
        self.db.commit()
        return articles
    
    def _parse_time(self, time_str):
        """解析时间字符串"""
        # 根据不同网站的时间格式进行解析
        try:
            return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
        except:
            return datetime.utcnow()
```

### 2.3 计算服务 (Calculation Service)

实现核心算法公式的计算功能。

```python
# calculation_service/service.py
import numpy as np
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from ..rule_service.models import Rule
from ..fund_service.models import Fund, FundNetValue
from ..news_service.models import NewsArticle
from ..database import get_db

class NetValueCalculator:
    def __init__(self, db: Session = None):
        self.db = db or next(get_db())
        self.alpha = 0.7  # 情感分析权重
        self.beta = 0.02  # 回归系数
        self.gamma = 0.003  # 随机波动系数
        self.max_net_value = 1.5  # 最大净值限制
        self.min_net_value = 0.5  # 最小净值限制
    
    def calculate_net_value(self, fund_id: str, news_id: str = None):
        """计算基金净值"""
        # 获取基金信息
        fund = self.db.query(Fund).filter(Fund.id == fund_id).first()
        if not fund:
            raise ValueError(f"Fund {fund_id} not found")
        
        # 获取上期净值
        last_net_value = self.db.query(FundNetValue).filter(
            FundNetValue.fund_id == fund_id
        ).order_by(FundNetValue.calculated_at.desc()).first()
        
        previous_net_value = last_net_value.net_value if last_net_value else 1.0
        
        # 计算新闻影响系数总和
        if news_id:
            news = self.db.query(NewsArticle).filter(NewsArticle.id == news_id).first()
            if news and news.fund_impact and fund_id in news.fund_impact:
                impact_sum = news.fund_impact[fund_id].get('impact_coefficient', 0)
            else:
                impact_sum = self._calculate_news_impact(fund_id, news_id)
        else:
            # 获取最近一段时间的新闻影响
            time_window = datetime.utcnow() - timedelta(minutes=30)
            recent_news = self.db.query(NewsArticle).filter(
                NewsArticle.published_at >= time_window,
                NewsArticle.fund_impact != None
            ).all()
            
            impact_sum = 0
            for news in recent_news:
                if fund_id in news.fund_impact:
                    impact_sum += news.fund_impact[fund_id].get('impact_coefficient', 0)
        
        # 计算虚拟调整因子
        target = 1.0  # 目标净值
        random_factor = np.random.uniform(-1, 1)
        adjustment_factor = self.beta * (target - previous_net_value) + self.gamma * random_factor
        
        # 计算当期净值
        current_net_value = previous_net_value * (1 + impact_sum + adjustment_factor)
        
        # 应用净值限制
        current_net_value = max(min(current_net_value, self.max_net_value), self.min_net_value)
        
        # 保存计算结果
        net_value_record = FundNetValue(
            id=str(uuid.uuid4()),
            fund_id=fund_id,
            previous_net_value=previous_net_value,
            current_net_value=current_net_value,
            impact_coefficient=impact_sum,
            adjustment_factor=adjustment_factor,
            news_id=news_id,
            calculated_at=datetime.utcnow()
        )
        
        self.db.add(net_value_record)
        self.db.commit()
        
        return current_net_value
    
    def _calculate_news_impact(self, fund_id: str, news_id: str):
        """计算新闻影响系数"""
        # 获取新闻信息
        news = self.db.query(NewsArticle).filter(NewsArticle.id == news_id).first()
        if not news:
            return 0
        
        # 获取基金类型
        fund = self.db.query(Fund).filter(Fund.id == fund_id).first()
        if not fund:
            return 0
        
        # 获取相关规则
        rules = self.db.query(Rule).filter(
            Rule.fund_type == fund.fund_type,
            Rule.status == "active"
        ).all()
        
        # 计算情感得分
        sentiment_score = self._calculate_sentiment_score(news, rules)
        
        # 计算重要性权重
        importance_weight = self._calculate_importance_weight(news, rules)
        
        # 计算关联性乘数
        relevance_multiplier = self._calculate_relevance_multiplier(news, fund.fund_type, rules)
        
        # 计算影响系数
        impact_coefficient = sentiment_score * importance_weight * relevance_multiplier
        
        # 更新新闻的影响系数
        if not news.fund_impact:
            news.fund_impact = {}
        
        news.fund_impact[fund_id] = {
            'impact_coefficient': impact_coefficient,
            'sentiment_score': sentiment_score,
            'importance_weight': importance_weight,
            'relevance_multiplier': relevance_multiplier,
            'calculated_at': datetime.utcnow().isoformat()
        }
        
        self.db.commit()
        
        return impact_coefficient
    
    def _calculate_sentiment_score(self, news: NewsArticle, rules: list):
        """计算情感得分 S = α × SA + (1-α) × SR"""
        # 自动情感分析得分 (这里使用TextBlob等工具)
        auto_score = self._get_auto_sentiment_score(news.content or news.title)
        
        # 规则匹配得分
        rule_score = self._get_rule_based_sentiment_score(news, rules)
        
        # 综合情感得分
        sentiment_score = self.alpha * auto_score + (1 - self.alpha) * rule_score
        
        # 确保得分在-1到1之间
        return max(min(sentiment_score, 1.0), -1.0)
    
    def _get_auto_sentiment_score(self, text: str):
        """获取自动情感分析得分"""
        try:
            from textblob import TextBlob
            analysis = TextBlob(text)
            return analysis.sentiment.polarity
        except:
            return 0
    
    def _get_rule_based_sentiment_score(self, news: NewsArticle, rules: list):
        """获取基于规则的情感得分"""
        sentiment_rules = [r for r in rules if r.rule_type == "sentiment"]
        if not sentiment_rules:
            return 0
        
        positive_matches = 0
        negative_matches = 0
        
        for rule in sentiment_rules:
            content = rule.content
            text = (news.title + " " + (news.content or "")).lower()
            
            # 检查正面规则
            for positive_rule in content.get('positive_rules', []):
                if positive_rule['pattern'].lower() in text:
                    positive_matches += 1
            
            # 检查负面规则
            for negative_rule in content.get('negative_rules', []):
                if negative_rule['pattern'].lower() in text:
                    negative_matches += 1
        
        # 计算规则得分 SR = (P - N) / (P + N + 1)
        if positive_matches + negative_matches == 0:
            return 0
        
        return (positive_matches - negative_matches) / (positive_matches + negative_matches + 1)
    
    def _calculate_importance_weight(self, news: NewsArticle, rules: list):
        """计算重要性权重 W"""
        importance_rules = [r for r in rules if r.rule_type == "impact"]
        if not importance_rules:
            return 1.0
        
        base_weight = 1.0
        text = (news.title + " " + (news.content or "")).lower()
        
        for rule in importance_rules:
            content = rule.content
            grade_rules = content.get('grade_rules', [])
            
            for grade_rule in grade_rules:
                # 简化的规则匹配逻辑
                if '国家级' in text and '100亿' in text:
                    return grade_rule.get('multiplier', 1.5)
                elif '省级' in text and ('50亿' in text or '100亿' in text):
                    return grade_rule.get('multiplier', 1.2)
                elif '市级' in text and '50亿' in text:
                    return grade_rule.get('multiplier', 1.0)
        
        return base_weight
    
    def _calculate_relevance_multiplier(self, news: NewsArticle, fund_type: str, rules: list):
        """计算关联性乘数 M"""
        keyword_rules = [r for r in rules if r.rule_type == "keyword"]
        if not keyword_rules:
            return 1.0
        
        text = (news.title + " " + (news.content or "")).lower()
        max_relevance = 0
        
        for rule in keyword_rules:
            content = rule.content
            primary_keywords = content.get('primary', [])
            secondary_keywords = content.get('secondary', [])
            
            # 检查一级关键词
            primary_matches = sum(1 for kw in primary_keywords if kw.lower() in text)
            if primary_matches == 0:
                continue
            
            # 检查二级关键词
            secondary_matches = sum(1 for kw in secondary_keywords if kw.lower() in text)
            
            # 计算关联性得分
            relevance_score = (primary_matches / len(primary_keywords)) * 0.7 + \
                             (secondary_matches / len(secondary_keywords)) * 0.3
            
            max_relevance = max(max_relevance, relevance_score)
        
        # 转换为乘数 (0.8-1.2)
        return 0.8 + (max_relevance * 0.4)
```

### 2.4 基金服务 (Fund Service)

实现基金信息的管理功能。

```python
# fund_service/models.py
from sqlalchemy import Column, String, Float, DateTime, Enum
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime
import enum

Base = declarative_base()

class FundType(str, enum.Enum):
    GREEN_INDUSTRY = "绿色产业引导基金"
    GREEN_BOND = "绿色债券基金"
    GREEN_STOCK = "绿色主题股票基金"
    ESG_INTEGRATED = "ESG整合型基金"
    BIOTECH_ECology = "生物科技生态基金"
    BLOCKCHAIN_ECOLOGY = "区块链赋能生态基金"

class FundStatus(str, enum.Enum):
    ACTIVE = "active"
    INACTIVE = "inactive"
    CLOSED = "closed"

class Fund(Base):
    __tablename__ = "funds"
    
    id = Column(String(36), primary_key=True)
    name = Column(String(100), nullable=False)
    fund_type = Column(Enum(FundType), nullable=False)
    description = Column(String(500))
    manager = Column(String(100))
    establishment_date = Column(DateTime, default=datetime.utcnow)
    status = Column(Enum(FundStatus), default=FundStatus.ACTIVE)
    total_assets = Column(Float, default=0)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class FundNetValue(Base):
    __tablename__ = "fund_net_values"
    
    id = Column(String(36), primary_key=True)
    fund_id = Column(String(36), nullable=False)
    previous_net_value = Column(Float, nullable=False)
    current_net_value = Column(Float, nullable=False)
    impact_coefficient = Column(Float, default=0)
    adjustment_factor = Column(Float, default=0)
    news_id = Column(String(36))
    calculated_at = Column(DateTime, default=datetime.utcnow)
```

## 三、API接口设计

### 3.1 核心API接口

```python
# api_gateway/main.py
from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy.orm import Session
import uvicorn
from .dependencies import get_db
from .routers import funds, rules, news, calculations, users

app = FastAPI(
    title="绿色生态基金虚拟净值模拟系统",
    description="基于现实新闻数据的绿色生态基金净值模拟系统API",
    version="1.0.0"
)

# CORS配置
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 注册路由
app.include_router(funds.router, prefix="/api/v1/funds", tags=["funds"])
app.include_router(rules.router, prefix="/api/v1/rules", tags=["rules"])
app.include_router(news.router, prefix="/api/v1/news", tags=["news"])
app.include_router(calculations.router, prefix="/api/v1/calculations", tags=["calculations"])
app.include_router(users.router, prefix="/api/v1/users", tags=["users"])

@app.get("/api/v1/health")
def health_check():
    """健康检查接口"""
    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}

if __name__ == "__main__":
    uvicorn.run("api_gateway.main:app", host="0.0.0.0", port=8000, reload=True)
```

```python
# api_gateway/routers/calculations.py
from fastapi import APIRouter, Depends, Query
from sqlalchemy.orm import Session
from datetime import datetime, timedelta
from typing import List, Optional
from ..dependencies import get_db
from ...calculation_service.service import NetValueCalculator
from ...fund_service.models import FundNetValue
from ...fund_service.schemas import FundNetValueResponse

router = APIRouter()

@router.post("/net-value/{fund_id}", response_model=float)
def calculate_fund_net_value(
    fund_id: str,
    news_id: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """计算基金净值"""
    calculator = NetValueCalculator(db)
    net_value = calculator.calculate_net_value(fund_id, news_id)
    return net_value

@router.get("/net-value/{fund_id}/history", response_model=List[FundNetValueResponse])
def get_net_value_history(
    fund_id: str,
    start_time: Optional[datetime] = None,
    end_time: Optional[datetime] = None,
    limit: int = Query(100, ge=1, le=1000),
    db: Session = Depends(get_db)
):
    """获取基金净值历史记录"""
    query = db.query(FundNetValue).filter(FundNetValue.fund_id == fund_id)
    
    if start_time:
        query = query.filter(FundNetValue.calculated_at >= start_time)
    if end_time:
        query = query.filter(FundNetValue.calculated_at <= end_time)
    
    history = query.order_by(FundNetValue.calculated_at.desc()).limit(limit).all()
    return history

@router.post("/news/{news_id}/impact", response_model=dict)
def calculate_news_impact(
    news_id: str,
    fund_ids: Optional[List[str]] = None,
    db: Session = Depends(get_db)
):
    """计算新闻对基金的影响"""
    calculator = NetValueCalculator(db)
    
    if not fund_ids:
        # 如果没有指定基金ID，计算对所有相关基金的影响
        from ...fund_service.models import Fund
        funds = db.query(Fund).filter(Fund.status == "active").all()
        fund_ids = [fund.id for fund in funds]
    
    results = {}
    for fund_id in fund_ids:
        impact = calculator._calculate_news_impact(fund_id, news_id)
        results[fund_id] = impact
    
    return results
```

## 四、数据持久层设计

### 4.1 数据库配置

```python
# database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os

# 数据库连接配置
DATABASE_URL = os.environ.get("DATABASE_URL", "mysql+pymysql://root:password@localhost:3306/green_fund")

# 创建数据库引擎
engine = create_engine(DATABASE_URL, pool_pre_ping=True, pool_recycle=300)

# 创建会话工厂
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# 基础模型类
Base = declarative_base()

def get_db():
    """获取数据库会话"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def create_tables():
    """创建数据库表"""
    Base.metadata.create_all(bind=engine)
```

### 4.2 Redis缓存配置

```python
# cache.py
import redis
import json
from datetime import timedelta
import os

class RedisCache:
    def __init__(self):
        self.host = os.environ.get("REDIS_HOST", "localhost")
        self.port = int(os.environ.get("REDIS_PORT", 6379))
        self.password = os.environ.get("REDIS_PASSWORD", "")
        self.db = int(os.environ.get("REDIS_DB", 0))
        self.client = self._connect()
    
    def _connect(self):
        """连接Redis"""
        try:
            return redis.Redis(
                host=self.host,
                port=self.port,
                password=self.password,
                db=self.db,
                decode_responses=True
            )
        except Exception as e:
            print(f"Redis connection error: {str(e)}")
            return None
    
    def get(self, key: str):
        """获取缓存数据"""
        if not self.client:
            return None
        
        try:
            data = self.client.get(key)
            if data:
                return json.loads(data)
            return None
        except Exception as e:
            print(f"Redis get error: {str(e)}")
            return None
    
    def set(self, key: str, value: dict, expire_seconds: int = 3600):
        """设置缓存数据"""
        if not self.client:
            return False
        
        try:
            self.client.setex(
                key,
                timedelta(seconds=expire_seconds),
                json.dumps(value)
            )
            return True
        except Exception as e:
            print(f"Redis set error: {str(e)}")
            return False
    
    def delete(self, key: str):
        """删除缓存数据"""
        if not self.client:
            return False
        
        try:
            self.client.delete(key)
            return True
        except Exception as e:
            print(f"Redis delete error: {str(e)}")
            return False
    
    def clear_pattern(self, pattern: str):
        """删除匹配模式的缓存"""
        if not self.client:
            return False
        
        try:
            keys = self.client.keys(pattern)
            if keys:
                self.client.delete(*keys)
            return True
        except Exception as e:
            print(f"Redis clear pattern error: {str(e)}")
            return False

# 创建缓存实例
cache = RedisCache()
```

## 五、系统部署与运维

### 5.1 Docker部署配置

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# 安装依赖
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 复制代码
COPY . .

# 设置环境变量
ENV PYTHONPATH=/app
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# 暴露端口
EXPOSE 8000

# 启动命令
CMD ["uvicorn", "api_gateway.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=mysql+pymysql://root:password@mysql:3306/green_fund
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      - mysql
      - redis
      - elasticsearch
    restart: always

  mysql:
    image: mysql:8.0
    ports:
      - "3306:3306"
    environment:
      - MYSQL_ROOT_PASSWORD=password
      - MYSQL_DATABASE=green_fund
    volumes:
      - mysql_data:/var/lib/mysql
    restart: always

  redis:
    image: redis:6.2
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: always

  elasticsearch:
    image: elasticsearch:7.14.0
    ports:
      - "9200:9200"
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    restart: always

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    restart: always

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    restart: always

volumes:
  mysql_data:
  redis_data:
  elasticsearch_data:
  prometheus_data:
  grafana_data:
```

### 5.2 监控配置

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'api'
    static_configs:
      - targets: ['api:8000']
    metrics_path: '/metrics'
    scrape_interval: 5s

  - job_name: 'mysql'
    static_configs:
      - targets: ['mysql:9104']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:9121']

  - job_name: 'elasticsearch'
    static_configs:
      - targets: ['elasticsearch:9114']
```

## 六、系统集成与测试

### 6.1 系统集成流程

1. **数据流程**：
   - 新闻爬虫定期抓取新闻数据
   - 规则服务对新闻进行分析和匹配
   - 计算服务根据规则和算法计算净值影响
   - 基金服务更新基金净值
   - API网关提供数据查询接口

2. **事件驱动**：
   - 新新闻到达时触发规则匹配
   - 规则匹配完成后触发净值计算
   - 净值更新后触发通知机制
   - 异常情况触发告警机制

### 6.2 测试策略

1. **单元测试**：
   - 测试各个服务的核心功能
   - 测试算法公式的正确性
   - 测试规则匹配的准确性

2. **集成测试**：
   - 测试服务间的交互
   - 测试数据流的完整性
   - 测试异常处理机制

3. **性能测试**：
   - 测试系统的并发处理能力
   - 测试响应时间和吞吐量
   - 测试系统的稳定性

## 七、总结与展望

### 7.1 系统特点

1. **完整性**：实现了从数据采集到结果展示的完整流程
2. **准确性**：基于规则和算法的精确计算
3. **可扩展性**：微服务架构支持系统扩展
4. **实时性**：支持实时数据处理和计算
5. **可靠性**：完善的监控和容错机制

### 7.2 未来优化方向

1. **AI增强**：引入更先进的机器学习算法
2. **多语言支持**：支持中英文等多语言新闻分析
3. **分布式计算**：支持大规模数据处理
4. **移动端应用**：开发移动应用客户端
5. **区块链集成**：探索区块链技术在基金管理中的应用

这个后端系统完整实现了基于规则虚拟库和算法公式的绿色生态基金虚拟净值模拟功能，为前端应用提供了强大的数据支持和计算能力。系统采用现代化的技术栈和架构设计，确保了系统的可扩展性、稳定性和性能。


## 八、用户服务开发记录

### 8.1 用户服务概述

用户服务是绿色生态基金虚拟净值模拟系统的核心微服务之一，负责用户管理、认证、资产管理和交易功能。

### 8.2 主要功能模块

1. **用户管理**：注册、登录、信息更新、状态管理
2. **资产管理**：余额查询、充值、提现
3. **持仓管理**：查询用户持仓信息
4. **交易管理**：买入/卖出基金、查询交易记录
5. **会话管理**：基于Redis的会话缓存

### 8.3 数据模型

```python
# user_service/models.py
class User(Base):
    id = Column(String(36), primary_key=True)
    username = Column(String(50), unique=True, nullable=False)
    email = Column(String(100), unique=True, nullable=False)
    password_hash = Column(String(255), nullable=False)
    phone = Column(String(20))
    balance = Column(Float, default=10000.0)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    last_login_at = Column(DateTime)
    status = Column(Enum(UserStatus), default=UserStatus.ACTIVE)
    user_type = Column(Enum(UserType), default=UserType.REGULAR)

class UserHolding(Base):
    id = Column(String(36), primary_key=True)
    user_id = Column(String(36), nullable=False)
    fund_id = Column(String(36), nullable=False)
    shares = Column(Float, default=0.0)
    purchase_cost = Column(Float, default=0.0)
    current_value = Column(Float, default=0.0)
    profit_loss = Column(Float, default=0.0)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class Transaction(Base):
    id = Column(String(36), primary_key=True)
    user_id = Column(String(36), nullable=False)
    fund_id = Column(String(36), nullable=False)
    transaction_type = Column(String(10), nullable=False)  # buy/sell
    amount = Column(Float, nullable=False)
    shares = Column(Float, nullable=False)
    unit_price = Column(Float, nullable=False)
    fee = Column(Float, default=0.0)
    net_amount = Column(Float, nullable=False)
    status = Column(String(20), default="pending")
    transaction_mode = Column(String(20))  # 交易模式
    scheduled_date = Column(DateTime)  # 预约交易日期
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime)
```

### 8.4 主要API端点

#### 用户认证
- `POST /users/register` - 用户注册
- `POST /users/login` - 用户登录

#### 用户信息
- `GET /users/{user_id}` - 获取用户信息（需要认证）
- `PUT /users/{user_id}` - 更新用户信息（需要认证）

#### 资产管理
- `POST /users/{user_id}/balance/deposit` - 充值余额
- `POST /users/{user_id}/balance/withdraw` - 提现余额

#### 持仓管理
- `GET /users/{user_id}/holdings` - 获取用户持仓列表

#### 交易管理
- `GET /users/{user_id}/transactions` - 获取用户交易记录
- `POST /transactions` - 创建交易（买入/卖出基金，需要认证）

### 8.5 技术实现要点

1. **安全机制**：
   - 密码使用SHA256哈希存储
   - 会话管理使用Redis缓存，有效期24小时
   - 使用依赖注入实现用户认证
   - 基于权限的资源访问控制

2. **交易处理**：
   - 实现了买入/卖出的完整逻辑
   - 包含手续费计算（0.5%）
   - 交易过程支持事务回滚
   - 自动更新用户余额和持仓信息

3. **数据库优化**：
   - 使用UUID作为主键，确保分布式环境下的唯一性
   - 交易记录支持分页查询
   - 数据模型设计考虑了索引优化

### 8.6 API网关集成

用户服务已集成到系统的API网关中，通过以下配置实现：

```python
# api_gateway/app.py
SERVICES = {
    "rule_service": "http://localhost:8001",
    "news_service": "http://localhost:8002",
    "calculation_service": "http://localhost:8003",
    "fund_service": "http://localhost:8004",
    "user_service": "http://localhost:8005"  # 用户服务
}

@app.route("/api/user_service/<path:path>")
async def proxy_to_user_service(path: str):
    return await proxy_request("user_service", path)
```

### 8.7 部署配置

环境变量配置：
- `PORT` - 服务端口，默认：8005
- `HOST` - 服务主机，默认：0.0.0.0
- `RELOAD` - 是否启用热重载，默认：true
- `DATABASE_URL` - 数据库连接URL
- `REDIS_HOST` - Redis主机，默认：localhost
- `REDIS_PORT` - Redis端口，默认：6379
- `REDIS_PASSWORD` - Redis密码（如无密码则为空）
- `REDIS_DB` - Redis数据库索引，默认：0

用户服务的完整实现为系统提供了强大的用户管理和交易功能，为前端应用提供了可靠的数据支持。